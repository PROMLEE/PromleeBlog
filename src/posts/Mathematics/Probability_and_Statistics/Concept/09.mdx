# Expectations of Standard Random Variables (표준확률변수의 기대값)
- 확률변수가 표준확률변수일 때, 기대값을 구하는 방법
1. Uniform(균등 분포) on ${a, a+1, ... , b} : E[X] = (a+b)/2$
2. Bernoulli(베르누이 분포): $E[X] = (1-p)*0+p*1=p$
3. Binomial(이항 분포): $E[X] = np$
4. Geometric(기하 분포): $E[X] = 1/p$
5. Poisson(포아송 분포): $E[X] = \lambda$

## Uniform Expectation (균등분포의 기대값 증명)
- $E[X] = (b+a)/2$
<Math>

```math
E[X] = \sum_{k=a}^{b} kP(X = k) \\
= \sum_{k=a}^{b} k \times \frac{1}{b - a + 1} \\
= \frac{1}{b - a + 1} \sum_{k=a}^{b} k \\
= \frac{1}{b - a + 1} \frac{(a + b)(b - a + 1)}{2} \\
= \frac{a + b}{2}
```
</Math>

- 설명
	- $E[X] = \frac{a + b}{2}$
	- $E[X] = \frac{1}{b - a + 1} \frac{(a + b)(b - a + 1)}{2}$
	- $E[X] = \frac{1}{b - a + 1} \sum_{k=a}^{b} k$
	- $E[X] = \sum_{k=a}^{b} k \times \frac{1}{b - a + 1}$
	- $E[X] = \frac{a + b}{2}$

## Poisson Expectation (포아송분포의 기대값 증명)
- $E[X] = \lambda$
<Math>

```math
E[X] = \sum_{k=0}^{\infty} k \cdot \frac{e^{-\lambda} \lambda^k}{k!} \\
= \lambda \sum_{k=1}^{\infty} \frac{k \cdot e^{-\lambda} \lambda^{k-1}}{k!} \\
= \lambda \sum_{k=1}^{\infty} \frac{e^{-\lambda} \lambda^{k-1}}{(k - 1)!} \\
= \lambda \left(P(X = 0) + P(X = 1) + P(X = 2) + \ldots \right) \\
= \lambda

```
</Math>

- 설명
	- $E[X] = \lambda \left(P(X = 0) + P(X = 1) + P(X = 2) + \ldots \right)$
	- $E[X] = \lambda \sum_{k=1}^{\infty} \frac{e^{-\lambda} \lambda^{k-1}}{(k - 1)!}$
	- $E[X] = \lambda \sum_{k=1}^{\infty} \frac{k \cdot e^{-\lambda} \lambda^{k-1}}{k!}$
	- $E[X] = \lambda \sum_{k=0}^{\infty} k \cdot \frac{e^{-\lambda} \lambda^k}{k!}$
	- $E[X] = \lambda$
## Geometric Expectation (기하분포의 기대값 증명)
- $E[X] = 1/p$
<Math>

```math
E[X] = \sum_{k=1}^{\infty} k \cdot (1 - p)^{k-1} \cdot p \\
= \sum_{k=1}^{\infty} (1 - p)^{k-1} \cdot p + \sum_{k=1}^{\infty} (k - 1) \cdot (1 - p)^{k-1} \cdot p \\
= 1 + (1 - p) \sum_{k=2}^{\infty} (k - 1) \cdot (1 - p)^{k-2} \cdot p \\
= 1 + (1 - p) \left( 1 \times P(X = 1) + 2 \times P(X = 2) + 3 \times P(X = 3) + \ldots \right) \\
= 1 + (1 - p) E[X] \\
\text{and so } E[X] = \frac{1}{p}
```
</Math>

- 설명
	- $E[X] = 1 + (1 - p) E[X]$ 에서 $E[X]$를 구하면 $E[X] = \frac{1}{p}$
	- $E[X] = 1 + (1 - p) E[X]$ 에서 $E[X]$를 구하는 방법은 다음과 같다.
		- $E[X] = 1 + (1 - p) \sum_{k=2}^{\infty} (k - 1) \cdot (1 - p)^{k-2} \cdot p$
		- $E[X] = 1 + (1 - p) \left( 1 \times P(X = 1) + 2 \times P(X = 2) + 3 \times P(X = 3) + \ldots \right)$
		- $E[X] = 1 + (1 - p) E[X]$

# 표준확률변수의 분산 (Variance of Standard Random Variables)
확률변수가 표준확률변수일 때, 분산을 구하는 방법
- Bernoulli(베르누이 분포): $var[X] = p(1 - p)$
- Binomial(이항 분포): $var[X] = np(1 - p)$
- Geometric(기하 분포): $var[X] = (1-p) / p^2$
- Uniform(균등 분포): $var[X] = (b-a+1)^2 - 1 / 12$
- Poisson(포아송 분포): $var[X] = λ$

[예제 1](#예제-1)


# 합동 확률 질량 함수 (Joint Probability Mass Function)
두 개의 이산 확률 변수 X, Y에 대한 합동 확률 질량 함수는 다음과 같이 정의된다.
$p_{X,Y}(x, y) = P(X = x, Y = y)$0
이 때, $p_{X,Y}(x, y)$는 X와 Y가 각각 x와 y를 가질 확률을 나타낸다.
$P(X = x, Y = y)$는 $P({X = x} \cap {Y = y})$의 축약형이다.

여러 속성을 실험의 결과 공간에서 설명 하는 것에 유용함
예를 들어, 무작위로 학생을 선택하고 X를 그들의 키로, Y를 그들의 시험 점수로 설정할 수 있다.
[예제 2](#예제-2)

# 주변 확률 질량 함수 (Marginal Probability Mass Function)
X와 Y의 확률 질량 함수를 다음 공식을 사용하여 계산 가능하다
$p_X(x) = P(X = x) = \sum_{y} P(X = x, Y = y)$
$p_Y(y) = P(Y = y) = \sum_{x} P(X = x, Y = y)$
이 때, $p_X(x)$는 X가 x를 가질 확률을 나타내고, $p_Y(y)$는 Y가 y를 가질 확률을 나타낸다.
$p_X$ 와 $p_Y$는 각각 X와 Y의 주변 확률 질량 함수(Marginal PMF)이다. -> 결합 PMF와 구별하기 위함
예를 들어, $p_X(x)$는 변수 Y의 모든 가능한 값에 대해 X가 특정 값 x를 취할 확률을 모두 합한 것이다.
이 과정을 통해 X의 독립적인 분포를 파악할 수 있으며, Y의 영향을 받지 않는 X의 행동을 이해하는 데 도움이 된다.
## Marginalization: 주변화
- 결합 확률 분포에서 주변 확률 분포를 계산하는 과정
$P(X = x) = \sum_{y} P(X = x, Y = y)$
$Y: \{y_1, y_2, \dots, y_N\}$
1. $\{Y = y_1\}, \{Y = y_2\}, \dots, \{Y = y_N\}$: disjoint events
2. $\{Y=y_1\} \cup \{Y=y_2\} \cup \dots \cup \{Y=y_N\} = \Omega$
=> $\{Y=y_1\}, \{Y=y_2\}, \dots, \{Y=y_N\}$: partigion of $\Omega$

$P_X(x) = P(\{X=x\}) = P(\{X=x\} \cap (\{Y=y_1\} \cup \{Y=y_2\} \cup \dots \cup \{Y=y_N\}))$
$= P(\{X=x\} \cap \{Y=y_1\}) + P(\{X=x\} \cap \{Y=y_2\}) + \dots + P(\{X=x\} \cap \{Y=y_N\})$
$= \sum_{y} P(\{X=x\} \cap \{Y=y\}) = \sum_{y} P(X=x, Y=y)$

[예제 3](#예제-3)

## 증명
<Math>

```math
p_X(x) = P(X = x)\\
= \sum_{y} P(X = x, Y = y) \\
\rightarrow \text{덧셈 공리(Additivity Axiom) \& 정규화 공리(Normalization Axiom) 사용}\\
= \sum_{y} p_{X,Y}(x, y)
```
</Math>
덧셈 공리는 여러 확률 이벤트의 확률을 합칠 때 사용되고, 정규화 공리는 전체 확률의 합이 1이 되도록 보장한다.


# 결합 분포에서 주변 분포 계산(Marginalization in Joint Distributions)
  - Y가 값 $j_1, j_2, \dots, j_N$을 가진다고 가정하면, 
    - $\{Y = j_1\}, \{Y = j_2\}, \dots, \{Y = j_N\}$으로 $\Omega$를 분할할 수 있다.
  - 따라서, $\{X = i\}$는 다음과 같이 분할될 수 있다.
    - $\{X = i\} \cap \{Y = j_1\}, \{X = i\} \cap \{Y = j_2\}, \dots, \{X = i\} \cap \{Y = j_N\}$
  - 그러므로,
    - $P(X = i) = P(\{X=i\})$
    - $= P(\{X = i\} \cap \{Y = j_1\}) + P(\{X = i\} \cap \{Y = j_2\}) + \dots + P(\{X = i\} \cap \{Y = j_N\})$
    - $= \sum_{j} P(\{X = i\} \cap \{Y = j\}) = \sum_{j} P(X = i, Y = j)$

### 추가 설명

- 이 예시에서는 Y 변수가 여러 값을 가질 수 있으며, X의 각 값에 대해 Y의 모든 가능한 값과의 결합 이벤트를 고려하여 X의 주변 확률을 계산하는 방법을 설명한다.
- 주변 확률을 계산할 때, 결합 확률 질량 함수에서 모든 Y의 값을 고려하여 X가 특정 값 i를 가질 확률을 합산한다.
- 이는 결합 확률 질량 함수를 통해 얻은 데이터를 이용해 각 변수의 독립적인 확률 분포를 파악하는 효과적인 방법이다.
- 결합된 데이터에서 각 변수의 독립적인 행동을 분석하려 할 때 주변 확률을 계산하는 것이 필수적이다.


# 여러 확률 변수의 함수(Functions of Multiple Random Variables)
- 함수의 기대값 규칙은 자연스럽게 확장되어 다음과 같은 형태를 취한다.
	- $E[g(X, Y)] = \sum_{x,y} g(x, y) p_{X,Y}(x, y)$
- 특별한 경우에, 함수 $g$가 선형이고 $Z = g(X, Y) = aX + bY + c$일 때, 주어진 스칼라 $a, b, c$에 대해 다음과 같다.
	- $E[aX + bY + c] = aE[X] + bE[Y] + c$


# 예제
## 예제 1
### 문제
- 당신의 확률론 수업에 300명의 학생이 있고, 각 학생이 다른 학생과 독립적으로 A를 받을 확률이 1/3이다. A를 받는 학생들의 기대 수는 어떻게 될까?
- i번째 학생이 A를 받으면 Xi = 1, 그렇지 않으면 0이다.
### 답
$X_i$: Bernoulli Random Variable
$P(X_i = 1) = 1/3, P(X_i = 0) = 2/3$

각 $X_1, X_2, ..., X_n$은 **Bernoulli Random Variable**이며 $E[X_i] = p = 1/3$이다.
분산 $var[X_i] = \underline{p(1 - p)} = \underline{1/3 * 2/3} = 2/9$
그들의 합 $X = X_1 + X_2 + ... + X_n$은 A를 받는 학생들의 수이며, n번의 독립 시행에서 '성공'의 수이다.
이것은 **Binomial Random Variables인** $n$과 $p$의 매개변수를 가진 이항 분포이다.
$\therefore E[X] = ΣE[Xi] = Σp = \underline{np} = \underline{300 * 1/3} = 100$이다.

### 추가 설명
이 예시에서 사용된 $X_1, X_2, ..., X_n$ 변수들은 베르누이 시행의 결과를 나타낸다.
베르누이 시행은 성공 또는 실패의 두 가지 결과만을 가진다.
이 경우 300명의 학생 각각이 A를 받을 확률이 독립적으로 1/3이므로, 베르누이 시행의 성공 확률 p는 1/3이다.
이항 분포의 기대값 공식을 사용하여, 전체 학생들 중 A를 받는 기대 학생 수를 계산할 수 있다.
이 때 n은 전체 학생 수인 300이고, p는 성공 확률인 1/3이므로 $np = 300 * (1/3) = 100$이 된다.
즉, 이 확률론 수업에서 평균적으로 100명의 학생이 A를 받을 것으로 기대할 수 있다.

## 예제 2
### 문제
결합 확률 질량 함수의 표 형식 표현
| X\\Y | Y=1  | Y=2  | Y=3  | Y=4  |
|-----|------|------|------|------|
| X=1 | 0.1  | 0.1  | 0    | 0.2  |
| X=2 | 0.05 | 0    | 0.1  | 0    |
| X=3 | 0    | 0.1  | 0.2  | 0.1  |
예를 들어, $P(X=2, Y=3) = 0.1, P(X=3, Y=1) = 0, ...$
### 답

### 추가 설명
- 이 표는 X와 Y의 각 값에 대해 결합 확률을 나타낸다.
- 각 셀의 값은 X와 Y가 해당 값으로 동시에 나타날 확률을 나타낸다.
- 예를 들어, $P(X=2, Y=3) = 0.1$은 X가 2이고 Y가 3일 확률이 0.1임을 의미한다.
- 표를 사용하면 각 변수 조합의 확률을 쉽게 찾아볼 수 있다.
- 이러한 표는 통계 분석에서 확률 변수들 간의 관계를 분석하는 데 중요한 도구이다.

## 예제 3
### 문제

- 예시: 주변 확률 질량 함수
  - 주변 확률 질량 함수의 표 형식 표현
    - 결합 확률 P(X, Y)
      - | X\\Y | 1    | 2    | 3    | 4    |
        |-----|------|------|------|------|
        | 1   | 0.1  | 0.1  | 0    | 0.2  |
        | 2   | 0.05 | 0.05 | 0.1  | 0    |
        | 3   | 0    | 0.1  | 0.2  | 0.1  |
    - X의 주변 확률 P(X)
      - | X   | P(X) |
        |-----|------|
        | 1   | 0.4  |
        | 2   | 0.2  |
        | 3   | 0.4  |
    - Y의 주변 확률 P(Y)
      - | Y   | P(Y) |
        |-----|------|
        | 1   | 0.15 |
        | 2   | 0.25 |
        | 3   | 0.3  |
        | 4   | 0.3  |

### 추가 설명

- 위 표에서, X와 Y의 결합 확률은 각 셀에 표시되며, 각 셀의 확률은 X와 Y가 동시에 해당 값들을 가질 확률을 나타낸다.
- X의 주변 확률은 Y의 모든 값에 대해 각 X 값의 결합 확률을 합산하여 계산된다.
  - 예를 들어, $P(X=1) = 0.1 + 0.1 + 0 + 0.2 = 0.4$
- Y의 주변 확률은 X의 모든 값에 대해 각 Y 값의 결합 확률을 합산하여 계산된다.
  - 예를 들어, $P(Y=1) = 0.1 + 0.05 + 0 = 0.15$
- 이러한 표는 통계적 분석에서 변수들의 독립적인 분포를 쉽게 이해하고 비교하는 데 도움을 준다.

### 또 다른 예시

| X\\Y | 1    | 2    | 3    | 4    |
|-----|------|------|------|------|
| 1   | 0.1  | 0.1  | 0    | 0  |
| 2   | 0 | 0.05 | 0.1  | 0.05 |
| 3   | 0.1  | 0.2  | 0.2  | 0.1

1. $P(X = 2, Y = 3)$의 값은 얼마인가?
- 표에서 $X = 2$이고 $Y = 3$인 셀의 값을 보면, 확률은 0.1이다.

2. $P(X = 3)$의 값은 얼마인가?
	- X가 3일 때의 주변 확률 $P(X = 3)$을 계산하기 위해서는 Y가 갖는 모든 값에 대해 $X = 3$의 결합 확률을 합산해야 한다.
	- 표를 참조하면,
		- $P(X = 3, Y = 1) = 0.1$
		- $P(X = 3, Y = 2) = 0.2$
		- $P(X = 3, Y = 3) = 0.2$
		- $P(X = 3, Y = 4) = 0.1$
- 이 확률들을 모두 합하면, $P(X = 3) = 0.1 + 0.2 + 0.2 + 0.1 = 0.6$이다.

이렇게 각 변수의 특정 값에 대한 결합 확률과 주변 확률을 통해 전체 확률 분포를 이해하고 계산할 수 있다.

### 추가문제
![240501-001602](/posts/240501-001602.png)